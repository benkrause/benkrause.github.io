<!DOCTYPE html>
<html>
<head>
<title>post</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<head>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<style>
body
{
    width: 100%;
    height: 100%;
    margin: 0px;
    padding: 0px;
    overflow-x: hidden; 
}
.navbar {
    overflow: hidden;
    background-color: #333;
    position: fixed; /* Set the navbar to fixed position */
    top: 10000; /* Position the navbar at the top of the page */
    width: 200%; /* Full width */
}
.navbar {
  overflow: hidden;
  background-color: #333;
  position: fixed;
  top: 0;
  width: 100%;
}

.navbar a {
  float: left;
  display: block;
  color: #f2f2f2;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 17px;
}

.navbar a:hover {
  background: #ddd;
  color: black;
}

.main {
  padding: 16px;
  margin-top: 30px;
  height: 1500px; /* Used in this example to enable scrolling */
}


h1{
    margin-top: 10px;
    margin-bottom: 10px;
    margin-right: 100px;
    margin-left: 100px;
}
img{
	margin-left: 0px;
    margin-top: 10px;
}
.image1{
	margin-left: 0px;
}
h2{
    margin-top: 10px;
    margin-bottom: 10px;
    margin-right: 100px;
    margin-left: 100px;
}
h3{
    margin-top: 10px;
    margin-bottom: 10px;
    margin-right: 100px;
    margin-left: 100px;
}
ul{
	margin-left: 100px;

}
ol{
	margin-left: 100px;

}
</style>

<div class="navbar">
  <a href="../../">Home</a>
  <a href="../../about">About</a>
  <a href="../../blog">Blog</a>
  <a href="../../publications">Publications</a>
  <a href="../../contact">Contact</a>
  <a class="active" href="https://github.com/benkrause" style="float:right;"><i class="fa fa-github"></i></a> 
  <a class="active" href="https://scholar.google.com/citations?user=ONNif60AAAAJ&hl=en&authuser=1" style="float:right; "><i class="ai ai-google-scholar-square ai-1"></i></a>  

</div>
<link rel="icon" href="https://benkrause.github.io/favicon.ico">
</head>
<p style="margin-top:200px;">

</p>
<h1>Achieving human-level text prediction</h1>
<p><p style="margin-left:100px;"></p>
<p style="margin-left:100px;">
Measuring the entropy/redundancy of the English language has long been an interest of linguists, psychologists, and information theorists alike. The true entropy of text is given by the generating distribution of the human writing the text. The entropy of text can be upper bounded by the cross-entropy of that text under a particular model, where model could be a human guesser or a probabilistic algorithm. In the past, it was generally assumed that humans could upper bound the entropy of text better other probabilistic models existing at the time; language is generated by humans after all. However, measuring the entropy of text under a human's language model is non-trivial, as humans don't just output probability distributions. A couple of well known past works have used text predictions from human subjects to yield estimates for the entropy of the English language that are widely cited today.
</p>
<h3>Estimating the entropy</h3>
<p style="margin-left:100px;">
The entropy of a discrete random variable is given by Claude Shannon's famous equation:
</p>
<p style="margin-left:100px;">
<img src="equation1.gif " width="400"  />
<!--<img src="http://latex.codecogs.com/gif.latex? H(X) = -\sum_i p(x_i) \; \log_{2} \; p(x_i) \;\;\;\;\;\;\;\;(1)" border="0"/>.-->
</p>
<p style="margin-left:100px;">
When referring to the entropy of English text, people generally mean the average conditional entropy of a character of text, measured in bits/character, given the history of text. The entropy of a distribution p can be upper bounded by using distribution q as an approximation to distribution p. The cross entropy of p approximated by q is given by:
</p>
<p style="margin-left:100px;">
<img src="equation2.gif " width="400" />
<!--<img src="http://latex.codecogs.com/gif.latex? H(X) = -\sum_i p(x_i) \; \log_{2} \; q(x_i) \;\;\;\;\;\;\;\;(2)" border="0"/>.-->
</p>
<p style="margin-left:100px;">
While exactly measuring the entropy of English text is impossible without the thought process of the human generating the text, a tighter upper bound can be established with a better approximating distribution. In this post, I compare how well recent deep learning models can upper bound the entropy of English text versus human subjects in past work.
</p>
<h2>The Shannon Game</h2>
<p style="margin-left:100px;">
In 1951, shortly after developing information theory, Shannon attempted to estimate the entropy of the English language using what is now known as the "Shannon game". Human subjects repeatedly guessed the next letter of text until they got it right, and then moved on to guessing the next letter. Shannon was able to use the number of guesses for each symbol to estimate bounds to the human-level entropy of text prediction. If the model (or human) guesses the next symbol in descending order of probability, the entropy of the data under the model can be upper bounded using the entropy of the distribution over the number of guesses it takes the model to predict the next symbol. <img src="symbol1.gif" class="image1"/>is the frequency that the model playing the Shannon game guesses the next symbol correctly on the i<sup>th</sup> guess (on the Nth character). The entropy of this distribution, which is the upper bound for the entropy of the generating distribution of the data, is given by:
</p>
<p style="margin-left:100px;">
<img src="equation3.gif " width="400"/>
<!--<img src="http://latex.codecogs.com/gif.latex? H(X) \leq - \sum_{i=1}^{27} q^N_i \; \log_{2} \; q^N_i \;\;\;\;\;\;\;\;(3)" border="0"/>.-->
</p>
<p style="margin-left:100px;">
This upper bound is achievable because the model could always assign probability <img src="symbol1.gif" class="image1"/>to its i<sup>th</sup> guess and it would achieve this cross-entropy. However, doing this would not allow the model to update the relative confidence of its guesses depending on its context, which is why this is only an upper bound. Using this formula with his human text predictors, Shannon estimated the upper bound of the entropy of English text to be ~1.3 bits/character. See Shannon's <a href="http://languagelog.ldc.upenn.edu/myl/Shannon1950.pdf">paper</a> for more details.
</p>
<!-- <img src="http://latex.codecogs.com/gif.latex? H(X) \geq - \sum_{i=1}^{27} i(q_i-q_{i+1}) \; log_{2} \; i \;\;\;\;\;\;\;\;(4)" border="0"/>. -->
<h2>Gambling estimate</h2>
<p style="margin-left:100px;">
The main drawback of the Shannon game is that models have no way to specify how confident they are in their predictions. In 1978, Cover and King devised a <a href="https://pdfs.semanticscholar.org/130b/1c7786328bf8f4ebea56e6d2f1cb992404ab.pdf">more precise way</a> to measure the human-level entropy of text, where subjects played a modified version of the Shannon game where they also waged bets on how confident they were in their guesses. Cover and King showed that an ideal gambler's wagers as a percentage of their current capital are proportional to the probability of the next symbol, and thus were able to establish a direct relationship between the return of the gambler and the entropy of the data under the gambler's model. Cover and King also provided the exact excerpt of text they used in their paper, making direct comparison with future work possible. They used 12 human subjects to measure the entropy from an excerpt taken from the book Jefferson the Virginian, (which was the same book but a different excerpt from what was used in Shannon's experiments). The subjects predicted the 75 characters of bold text (case insensitive and excluding punctuation) with the help of statistical models, and were given access to the entire book up to this excerpt to allow them to adapt to the author's style. The subjects, made up of students and professors at Stanford, were apparently very dedicated to achieving the best performance possible, spending an average of 5 hours each at a computer terminal to make predictions on just 75 characters of text.

</p>
<p style="margin-left:150px;">She was not only a “pretty lady” but an accomplished one in the
customary ways, and her love for music was a special bond with him.
She played on the harpsichord and the pianoforte, as he did on the
violin and the cello. The tradition is that music provided the accompaniment
for his successful suit: his rivals are said to have departed in
admitted defeat after hearing him play and sing with her . In later
years he had the cheerful habit of singing and humming to himself as
he went about his plantation. This is not proof in its<b>elf that he was a
pleasing vocal performer, but with Martha in the parlor it</b></p>
<h2>Comparison with state of the art character models</h2>
<p style="margin-left:100px;">
I apply some recent state of the art advances in character-level language modeling from my own work to compare with the human prediction results from Cover and King. I used model pretrained on the text8 dataset, which is a commonly used character-level prediction benchmark. The model uses 2 recent advances:

</p>
<ol>
<li>
<p><a href="https://arxiv.org/abs/1609.07959">Multiplicative LSTM</a>- This is an LSTM modification that uses additional input dependent gates, which
modify the recurrent weight matrix depending on the input symbol. The edges represent multiplication by a matrix, and the inputs to the square nodes are multiplied, allowing for the effective recurrent weight matrix to be different for each possible input symbol.</p>
<p><center><img src="mRNN3.png " width="250"/></center></p>
</li>
<li>
<p><a href="https://arxiv.org/abs/1709.07432">Dynamic evaluation</a>- This is an adaptation technique for capturing re-occurring patterns in sequences, and is useful for modeling the rare words or style of a particular sequence. When used for sequence prediction, the model predicts a distribution over the next  sequence element, and then does a gradient descent update on that sequence element before predicting the next sequence element. An unrolled RNN is shown in the figure. A standard RNN uses fixed weights and only uses a hidden state to summarize the sequence history, whereas a dynamically evaluated RNN also uses gradient information stored in dynamically changing weights. </p>
<p><center><img src="dynamiceval-1.png " width="400"/></center></p>
</li>
</ol>
<p style="margin-left:100px;">
The distribution of the training set and test set are noticeably different, as text8 is taken from modern Wikipedia, whereas the excerpt is from a book written in 1948. To partially make up for this, I gave the entire book up until the test sequence as conditioning text to allow the model to adapt to the author's style. Perhaps better results could be achieved with a full retraining on a dataset of text from this author or time-period. Here were the results, in comparison to the human level results given by Cover and King's paper.
</p>
<p style="margin-left:100px;">
<table style="border-collapse:collapse;margin-left:100px;">

  <tr>
    <th>method</th>
    <th>cross-entropy (bits/char)</th>
  </tr>
  </tr>  <tr>
    <td>Human (avg subject)</td>
    <td>1.59</td>
  </tr>  <tr>
  <tr>
    <td>Human (based on avg gambling return)</td>
    <td>1.34</td>
  </tr>
  <tr>
    <td>Human (best subject out of 12)</td>
    <td>1.29</td>
  </tr>  <tr>
    <td>Human (ensemble of 12)</td>
    <td>1.25</td>
  </tr>  <tr>
  </tr>  <tr>
    <td>mLSTM (without dynamic eval)</td>
    <td>1.44</td>
  </tr>  <tr>
    <td>mLSTM (with dynamic eval)</td>
    <td>1.31</td>
</table>
</p>
<h2>Discussion</h2>
<p style="margin-left:100px;">
State of the art character-level language models performed on par with the best human text predictors, both achieving cross-entropies of ~1.3 bits/character on the tested text excerpt. It is likely that deep learning models and human models make different kinds of errors; for instance, humans are much better at understanding the higher level structure and logic of the text, whereas deep learning models may be better at knowing their own confidence, and may capture certain statistical regularities that humans fail to notice. Deep learning models also don't have to worry about becoming impatient or bored during monotonous tasks. Assuming human and deep learning models make different kinds of mistakes, it is likely they could be ensembled or combined in other ways to further reduce the upper bound on the entropy of text.
</p>
<h3>Other relevant work</h3>
<ul>
<li>
<p>Some of the first work to rigorously <a href="https://dl.acm.org/citation.cfm?id=146685">upper bound the entropy of English</a> using probabilistic models used a combination of character models and trigram word models, and achieved a cross entropy of 1.75 bit/character. </p>
</li>
<li>
<p>Work in 2002 <a href="https://pdfs.semanticscholar.org/ea49/699760a3a5554e2d785619c75d54183b35f5.pdf">examined the relative performance</a> of humans vs. statistical models at word prediction. </p>
</li>
<li>
<p>In addition to the advances used in this post from my own work, other recent work has also helped improve the state of the art in language modeling, including improvements to regularization such as given in <a href="https://arxiv.org/pdf/1708.02182.pdf">AWD-LSTM</a>, and expressiveness of language models as given in the <a href="https://arxiv.org/pdf/1711.03953.pdf">Mixture of Softmaxes</a> method.</p>
</li>
</ul>
<h3>Acknowledgments</h3>
<ul>
<li>A special thanks to <a href="https://twitter.com/mannykayy">Manny Kahembwe</a> for encouraging me to write this post, and to <a href="https://twitter.com/driainmurray">Iain Murray</a> and <a href="https://twitter.com/srenals">Steve Renals</a> for their advice and comments.</li>
</ul>
<p><body style = "background-image: url('../../images/neurons2.jpg');background-repeat: no-repeat; background-position: right 50px ;background-size: 100% 140px;"></p>
<h3>Code</h3>
<ul>
<li>If you want to play around with the (very messy) code I used for this post, I made it available <a href="https://github.com/benkrause/human-level-text-prediction">here</a>.</li>
</ul>
<p><br/>
<br/></p>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
